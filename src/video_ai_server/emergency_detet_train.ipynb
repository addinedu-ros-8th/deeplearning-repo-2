{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743765691.188555    6368 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743765691.197117    6368 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743765691.226496    6368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743765691.226631    6368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743765691.226635    6368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743765691.226638    6368 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your system is cuda\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import platform\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch.cuda import empty_cache\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    print(\"your system is mac os\")\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "else:\n",
    "    print(\"your system is cuda\")\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO-POSE 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "yolo_best_model = '/home/pepsi/dev_ws/deeplearning-repo-2/src/video_ai_server/models/extin_per_fire.pt'\n",
    "yolo_model = YOLO(yolo_best_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_YOLO_box(img, yolo_model, detect_cls):\n",
    "    box_results = yolo_model.predict(img, conf=0.6, verbose=False, show=False)\n",
    "    box_results = box_results[0].boxes\n",
    "\n",
    "    boxes = box_results.xyxy.cpu().tolist()\n",
    "    box_class = box_results.cls.cpu().tolist()\n",
    "\n",
    "    p1x1, p1y1, p1x2, p1y2 = 0, 0, 0, 0\n",
    "    p2x1, p2y1, p2x2, p2y2 = 0, 0, 0, 0\n",
    "    for idx, cls in enumerate(box_class):\n",
    "        if int(cls) == detect_cls:\n",
    "            p1x1, p1y1, p1x2, p1y2 = boxes[0]\n",
    "            p1x1, p1y1, p1x2, p1y2 = int(p1x1), int(p1y1), int(p1x2), int(p1y2)\n",
    "\n",
    "            if len(boxes) > 1:\n",
    "                p2x1, p2y1, p2x2, p2y2 = boxes[1]\n",
    "                p2x1, p2y1, p2x2, p2y2 = int(p2x1), int(p2y1), int(p2x2), int(p2y2)\n",
    "\n",
    "    return p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_landmarks(results):\n",
    "    xyz_list = []\n",
    "    for landmark in results.pose_landmarks.landmark:\n",
    "        xyz_list.append(landmark.x)\n",
    "        xyz_list.append(landmark.y)\n",
    "        xyz_list.append(landmark.z)\n",
    "    return xyz_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_xyz_list_list(xyz_list_list, p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2, xyz_list):\n",
    "    xyz_list.append(abs(p1x1 - p2x1) / 640)\n",
    "    xyz_list.append(abs(p1x2 - p2x2) / 640)\n",
    "    xyz_list.append(abs(p1y1 - p2y1) / 640)\n",
    "    xyz_list.append(abs(p1y2 - p2y2) / 640)\n",
    "    xyz_list_list.append(xyz_list)\n",
    "\n",
    "    return xyz_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(mp_pose, video_path, detect_cls):\n",
    "    xyz_list_list = []\n",
    "    poses = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "            if ret == True:\n",
    "                xyz_list = []\n",
    "                img = cv2.resize(img, (640, 640))\n",
    "                results = poses.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                if not results.pose_landmarks: continue\n",
    "\n",
    "                xyz_list = get_pose_landmarks(results)\n",
    "                p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2 = get_YOLO_box(img, yolo_model, detect_cls)\n",
    "\n",
    "                if (p1x1 == 0 and p1y1 == 0 and p1x2 == 0 and p1y2== 0) and (p2x1 == 0 and p2y1 == 0 and p2x2 == 0 and p2y2== 0): continue\n",
    "\n",
    "                xyz_list_list = append_data_xyz_list_list(xyz_list_list, p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2, xyz_list)\n",
    "\n",
    "                cv2.waitKey(1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return xyz_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743665825.956175   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743665825.960872   17507 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743665826.074159   17497 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665826.147130   17495 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665826.176504   17502 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "  6%|▋         | 1/16 [00:32<08:00, 32.03s/it]I0000 00:00:1743665857.916477   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743665857.917805   17536 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743665858.044154   17527 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665858.107029   17529 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 12%|█▎        | 2/16 [00:56<06:24, 27.46s/it]I0000 00:00:1743665882.171828   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743665882.173666   17564 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743665882.300804   17553 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665882.363446   17550 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 19%|█▉        | 3/16 [01:21<05:44, 26.53s/it]I0000 00:00:1743665907.601647   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743665907.603104   17594 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743665907.724011   17580 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665907.800201   17584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 25%|██▌       | 4/16 [01:52<05:40, 28.40s/it]I0000 00:00:1743665938.870343   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743665938.879831   17625 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743665939.048974   17612 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665939.100331   17618 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 31%|███▏      | 5/16 [02:20<05:09, 28.13s/it]I0000 00:00:1743665966.521464   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743665966.524855   17687 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743665966.696336   17675 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665966.746237   17677 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 38%|███▊      | 6/16 [02:51<04:49, 29.00s/it]I0000 00:00:1743665997.197095   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743665997.200891   17732 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743665997.379244   17726 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743665997.524466   17722 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 44%|████▍     | 7/16 [03:19<04:18, 28.71s/it]I0000 00:00:1743666025.308289   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666025.309688   17776 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666025.455678   17769 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666025.506311   17768 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 50%|█████     | 8/16 [03:43<03:38, 27.32s/it]I0000 00:00:1743666049.649126   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666049.650523   17804 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666049.762504   17793 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666049.824127   17796 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 56%|█████▋    | 9/16 [04:07<03:04, 26.29s/it]I0000 00:00:1743666073.697465   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666073.699534   17834 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666073.824820   17826 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666073.881399   17824 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 62%|██████▎   | 10/16 [04:33<02:36, 26.02s/it]I0000 00:00:1743666099.086142   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666099.087556   17862 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666099.211798   17851 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666099.271144   17856 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 69%|██████▉   | 11/16 [04:57<02:08, 25.62s/it]I0000 00:00:1743666123.793555   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666123.795208   17890 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666123.926188   17879 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666124.002572   17883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 75%|███████▌  | 12/16 [05:23<01:42, 25.58s/it]I0000 00:00:1743666149.295246   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666149.298673   17921 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666149.421993   17909 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666149.484772   17911 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 81%|████████▏ | 13/16 [05:46<01:14, 24.83s/it]I0000 00:00:1743666172.409458   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666172.411051   17949 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666172.576598   17938 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666172.651125   17939 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 88%|████████▊ | 14/16 [06:13<00:50, 25.37s/it]I0000 00:00:1743666199.037008   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666199.041122   17979 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666199.167057   17968 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666199.236124   17967 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 94%|█████████▍| 15/16 [06:38<00:25, 25.48s/it]I0000 00:00:1743666224.781503   17200 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743666224.787770   18010 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743666224.992873   17996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743666225.137197   18004 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|██████████| 16/16 [07:24<00:00, 27.78s/it]\n"
     ]
    }
   ],
   "source": [
    "Video_path = '/home/pepsi/dev_ws/deeplearning-repo-2/src/video_ai_server/datasets/pose/train2'\n",
    "video_name_list = os.listdir(Video_path)\n",
    "dataset = []\n",
    "length = 18\n",
    "detect_cls = 1\n",
    "\n",
    "for video_name in tqdm(video_name_list):\n",
    "    if 'normal' in video_name: label = 0\n",
    "    elif 'fighting' in video_name: label = 1\n",
    "    elif 'lying' in video_name: label = 2\n",
    "    elif 'smoking' in video_name: label = 3\n",
    "\n",
    "    pose_data = generate_dataset(mp_pose, '{}/{}'.format(Video_path, video_name), detect_cls)\n",
    "\n",
    "    for idx in range(0, len(pose_data), int(length)):\n",
    "        seq_list = pose_data[idx : idx + length]\n",
    "        if len(seq_list) == length:\n",
    "            dataset.append({'key' : label, 'value': seq_list})\n",
    "\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq_list):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for dic in seq_list:\n",
    "            self.y.append(dic['key'])\n",
    "            self.X.append(dic['value'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = [0.7, 0.2, 0.1]\n",
    "train_len = int(len(dataset) * split_ratio[0])\n",
    "val_len = int(len(dataset) * split_ratio[1])\n",
    "test_len = len(dataset) - train_len - val_len\n",
    "\n",
    "train_dataset = MyDataset(dataset)\n",
    "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8)\n",
    "val_loader = DataLoader(valid_data, batch_size=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(103, 128, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(256)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(256, 64, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm2 = nn.LayerNorm(128)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm3 = nn.LSTM(128, 32, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm3 = nn.LayerNorm(64)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "        self.attention = nn.Linear(64, 1)\n",
    "        self.fc = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.layer_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
    "        x = torch.sum(attention_weights * x, dim=1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    global net, loss_fn, optim\n",
    "    net = LSTM().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optim = AdamW(net.parameters(), lr = 0.001)\n",
    "\n",
    "def init_epoch():\n",
    "    global epoch_cnt\n",
    "    epoch_cnt = 0\n",
    "\n",
    "def init_log():\n",
    "    global iter_log, tloss_log, tacc_log, vloss_log, vacc_log, log_stack, time_log\n",
    "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
    "    log_stack, time_log = [], []\n",
    "\n",
    "def record_train_log(_tloss, _tacc, _time):\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    tacc_log.append(_tacc)\n",
    "    iter_log.append(epoch_cnt)\n",
    "\n",
    "def record_valid_log(_vloss, _vacc):\n",
    "    vloss_log.append(_vloss)\n",
    "    vacc_log.append(_vacc)\n",
    "\n",
    "def last(log_list):\n",
    "    if len(log_list) > 0:\n",
    "        return log_list[len(log_list) - 1]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def print_log():\n",
    "    train_loss = round(float(last(tloss_log)), 3)\n",
    "    train_acc = round(float(last(tacc_log)), 3)\n",
    "    val_loss = round(float(last(vloss_log)), 3)\n",
    "    val_acc = round(float(last(vacc_log)), 3)\n",
    "    time_spent = round(float(last(time_log)), 3)\n",
    "\n",
    "    log_str = 'Epoch: {:3}| T_Loss {:5} | T_acc {:5}| V_Loss {:5}| V_acc {:5} | {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)\n",
    "\n",
    "    log_stack.append(log_str) #프린트 준비\n",
    "\n",
    "    for idx in reversed(range(len(log_stack))):\n",
    "        print(log_stack[idx])\n",
    "\n",
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "def epoch_not_finished():\n",
    "    return epoch_cnt < maximum_epoch\n",
    "\n",
    "\n",
    "def epoch(data_loader, mode='train'):\n",
    "    global epoch_cnt\n",
    "\n",
    "    iter_loss, iter_acc, last_grad_performed = [], [], False\n",
    "\n",
    "    for _data, _label in data_loader:\n",
    "        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)\n",
    "\n",
    "        if mode == 'train' : net.train()\n",
    "        else: net.eval()\n",
    "\n",
    "        result = net(data)\n",
    "        _, out = torch.max(result, 1)\n",
    "\n",
    "        loss = loss_fn(result, label)\n",
    "        iter_loss.append(loss.item())\n",
    "\n",
    "        if mode == 'train':\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            last_grad_performed = True\n",
    "\n",
    "        acc_partial = (out == label).float().sum()\n",
    "        acc_partial = acc_partial / len(label)\n",
    "        iter_acc.append(acc_partial.item())\n",
    "\n",
    "\n",
    "    if last_grad_performed:\n",
    "        epoch_cnt += 1\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "    return np.average(iter_loss), np.average(iter_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  44| T_Loss 0.092 | T_acc 0.967| V_Loss 0.477| V_acc 0.839 | 1.775\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  45| T_Loss   0.2 | T_acc 0.902| V_Loss 0.026| V_acc   1.0 |  2.22\n",
      "Epoch:  44| T_Loss 0.092 | T_acc 0.967| V_Loss 0.477| V_acc 0.839 | 1.775\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  46| T_Loss  0.06 | T_acc 0.984| V_Loss 0.024| V_acc   1.0 | 1.902\n",
      "Epoch:  45| T_Loss   0.2 | T_acc 0.902| V_Loss 0.026| V_acc   1.0 |  2.22\n",
      "Epoch:  44| T_Loss 0.092 | T_acc 0.967| V_Loss 0.477| V_acc 0.839 | 1.775\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  47| T_Loss  0.04 | T_acc 0.984| V_Loss   0.2| V_acc 0.875 | 2.125\n",
      "Epoch:  46| T_Loss  0.06 | T_acc 0.984| V_Loss 0.024| V_acc   1.0 | 1.902\n",
      "Epoch:  45| T_Loss   0.2 | T_acc 0.902| V_Loss 0.026| V_acc   1.0 |  2.22\n",
      "Epoch:  44| T_Loss 0.092 | T_acc 0.967| V_Loss 0.477| V_acc 0.839 | 1.775\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  48| T_Loss 0.109 | T_acc 0.951| V_Loss 0.031| V_acc 0.982 | 2.082\n",
      "Epoch:  47| T_Loss  0.04 | T_acc 0.984| V_Loss   0.2| V_acc 0.875 | 2.125\n",
      "Epoch:  46| T_Loss  0.06 | T_acc 0.984| V_Loss 0.024| V_acc   1.0 | 1.902\n",
      "Epoch:  45| T_Loss   0.2 | T_acc 0.902| V_Loss 0.026| V_acc   1.0 |  2.22\n",
      "Epoch:  44| T_Loss 0.092 | T_acc 0.967| V_Loss 0.477| V_acc 0.839 | 1.775\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  49| T_Loss 0.051 | T_acc 0.978| V_Loss 0.066| V_acc 0.982 | 1.481\n",
      "Epoch:  48| T_Loss 0.109 | T_acc 0.951| V_Loss 0.031| V_acc 0.982 | 2.082\n",
      "Epoch:  47| T_Loss  0.04 | T_acc 0.984| V_Loss   0.2| V_acc 0.875 | 2.125\n",
      "Epoch:  46| T_Loss  0.06 | T_acc 0.984| V_Loss 0.024| V_acc   1.0 | 1.902\n",
      "Epoch:  45| T_Loss   0.2 | T_acc 0.902| V_Loss 0.026| V_acc   1.0 |  2.22\n",
      "Epoch:  44| T_Loss 0.092 | T_acc 0.967| V_Loss 0.477| V_acc 0.839 | 1.775\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "Epoch:  50| T_Loss 0.046 | T_acc 0.989| V_Loss 0.054| V_acc 0.982 | 0.972\n",
      "Epoch:  49| T_Loss 0.051 | T_acc 0.978| V_Loss 0.066| V_acc 0.982 | 1.481\n",
      "Epoch:  48| T_Loss 0.109 | T_acc 0.951| V_Loss 0.031| V_acc 0.982 | 2.082\n",
      "Epoch:  47| T_Loss  0.04 | T_acc 0.984| V_Loss   0.2| V_acc 0.875 | 2.125\n",
      "Epoch:  46| T_Loss  0.06 | T_acc 0.984| V_Loss 0.024| V_acc   1.0 | 1.902\n",
      "Epoch:  45| T_Loss   0.2 | T_acc 0.902| V_Loss 0.026| V_acc   1.0 |  2.22\n",
      "Epoch:  44| T_Loss 0.092 | T_acc 0.967| V_Loss 0.477| V_acc 0.839 | 1.775\n",
      "Epoch:  43| T_Loss 0.141 | T_acc 0.951| V_Loss 0.075| V_acc 0.964 | 1.784\n",
      "Epoch:  42| T_Loss 0.095 | T_acc 0.971| V_Loss 0.416| V_acc 0.893 | 2.381\n",
      "Epoch:  41| T_Loss 0.093 | T_acc 0.967| V_Loss 0.152| V_acc 0.982 | 0.374\n",
      "Epoch:  40| T_Loss 0.267 | T_acc 0.886| V_Loss 0.047| V_acc   1.0 | 0.519\n",
      "Epoch:  39| T_Loss 0.149 | T_acc 0.962| V_Loss 0.017| V_acc   1.0 | 0.859\n",
      "Epoch:  38| T_Loss 0.078 | T_acc 0.967| V_Loss 0.031| V_acc   1.0 | 1.286\n",
      "Epoch:  37| T_Loss 0.086 | T_acc 0.973| V_Loss 0.138| V_acc 0.946 |  0.58\n",
      "Epoch:  36| T_Loss 0.174 | T_acc 0.918| V_Loss 0.255| V_acc 0.929 | 0.413\n",
      "Epoch:  35| T_Loss 0.094 | T_acc 0.973| V_Loss 0.289| V_acc 0.893 | 0.436\n",
      "Epoch:  34| T_Loss 0.095 | T_acc 0.951| V_Loss 0.204| V_acc 0.946 | 1.319\n",
      "Epoch:  33| T_Loss 0.028 | T_acc 0.995| V_Loss 0.363| V_acc 0.893 | 0.343\n",
      "Epoch:  32| T_Loss 0.067 | T_acc 0.978| V_Loss 0.042| V_acc 0.982 |  0.62\n",
      "Epoch:  31| T_Loss 0.122 | T_acc 0.957| V_Loss 0.102| V_acc 0.964 | 0.383\n",
      "Epoch:  30| T_Loss 0.102 | T_acc 0.957| V_Loss  0.15| V_acc 0.946 | 1.415\n",
      "Epoch:  29| T_Loss 0.126 | T_acc  0.94| V_Loss 0.163| V_acc 0.946 | 0.645\n",
      "Epoch:  28| T_Loss 0.135 | T_acc 0.967| V_Loss 0.211| V_acc 0.946 | 0.336\n",
      "Epoch:  27| T_Loss 0.231 | T_acc  0.88| V_Loss 0.141| V_acc 0.964 | 0.335\n",
      "Epoch:  26| T_Loss 0.389 | T_acc 0.859| V_Loss 0.184| V_acc 0.946 |  0.44\n",
      "Epoch:  25| T_Loss 0.035 | T_acc 0.995| V_Loss 0.031| V_acc   1.0 |  0.58\n",
      "Epoch:  24| T_Loss 0.076 | T_acc 0.978| V_Loss 0.029| V_acc   1.0 | 3.428\n",
      "Epoch:  23| T_Loss 0.203 | T_acc 0.913| V_Loss 0.188| V_acc 0.929 | 0.301\n",
      "Epoch:  22| T_Loss 0.222 | T_acc 0.891| V_Loss 0.624| V_acc 0.768 | 0.292\n",
      "Epoch:  21| T_Loss 0.309 | T_acc 0.875| V_Loss 0.483| V_acc 0.804 | 0.283\n",
      "Epoch:  20| T_Loss 0.245 | T_acc 0.918| V_Loss 0.585| V_acc  0.75 | 0.373\n",
      "Epoch:  19| T_Loss 0.097 | T_acc 0.978| V_Loss 0.075| V_acc 0.982 | 0.413\n",
      "Epoch:  18| T_Loss 0.183 | T_acc  0.94| V_Loss 0.205| V_acc 0.929 | 1.285\n",
      "Epoch:  17| T_Loss 0.264 | T_acc 0.902| V_Loss 0.513| V_acc 0.768 | 1.777\n",
      "Epoch:  16| T_Loss 0.357 | T_acc 0.815| V_Loss 0.344| V_acc 0.839 | 0.603\n",
      "Epoch:  15| T_Loss 0.446 | T_acc 0.694| V_Loss 0.393| V_acc 0.768 | 1.645\n",
      "Epoch:  14| T_Loss 0.444 | T_acc 0.841| V_Loss  0.48| V_acc 0.732 |  0.59\n",
      "Epoch:  13| T_Loss 0.113 | T_acc 0.967| V_Loss  0.08| V_acc 0.982 | 0.575\n",
      "Epoch:  12| T_Loss 0.154 | T_acc 0.946| V_Loss 0.106| V_acc 0.964 | 0.631\n",
      "Epoch:  11| T_Loss 0.309 | T_acc 0.853| V_Loss 0.324| V_acc 0.821 | 0.823\n",
      "Epoch:  10| T_Loss 0.259 | T_acc 0.884| V_Loss 0.521| V_acc  0.75 | 1.134\n",
      "Epoch:   9| T_Loss 0.289 | T_acc 0.862| V_Loss 0.259| V_acc 0.857 |  2.04\n",
      "Epoch:   8| T_Loss 0.425 | T_acc 0.766| V_Loss 0.339| V_acc 0.857 |  1.65\n",
      "Epoch:   7| T_Loss 0.451 | T_acc 0.705| V_Loss 0.396| V_acc  0.75 | 1.495\n",
      "Epoch:   6| T_Loss   0.3 | T_acc 0.897| V_Loss 0.428| V_acc 0.786 | 1.503\n",
      "Epoch:   5| T_Loss 0.367 | T_acc 0.793| V_Loss  0.34| V_acc 0.821 | 1.328\n",
      "Epoch:   4| T_Loss 0.458 | T_acc 0.732| V_Loss 0.394| V_acc 0.732 |  2.84\n",
      "Epoch:   3| T_Loss 0.533 | T_acc 0.687| V_Loss 0.473| V_acc  0.75 | 0.834\n",
      "Epoch:   2| T_Loss 0.586 | T_acc 0.739| V_Loss 0.476| V_acc 0.786 | 2.517\n",
      "Epoch:   1| T_Loss 1.123 | T_acc 0.511| V_Loss  0.64| V_acc 0.732 | 1.363\n",
      "\n",
      " Training completed!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "maximum_epoch = 50\n",
    "\n",
    "while epoch_not_finished():\n",
    "    start_time = time.time()\n",
    "    tloss, tacc = epoch(train_loader, mode='train')\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, tacc, time_taken)\n",
    "    with torch.no_grad():\n",
    "        vloss, vacc = epoch(val_loader, mode= 'val')\n",
    "        record_valid_log(vloss, vacc)\n",
    "    print_log()\n",
    "\n",
    "print('\\n Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 1.0\n",
      "Test Loss: 0.009\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_loss, test_acc = epoch(test_loader, mode='test')\n",
    "    test_acc = round(test_acc, 4)\n",
    "    test_loss = round(test_loss, 4)\n",
    "    print('Test Acc: {}'.format(test_acc))\n",
    "    print('Test Loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/lstm_model.pth'\n",
    "\n",
    "torch.save(net.state_dict(), model_path)\n",
    "print('모델이 저장되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "REC_IP = \"192.168.0.85\"\n",
    "REC_PORT = 5001\n",
    "\n",
    "rec_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "rec_socket.connect((REC_IP, REC_PORT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743766246.185763    6368 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743766246.187532    6831 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743766246.261271    6818 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743766246.327719    6819 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/lstm_model.pth'\n",
    "\n",
    "lstm_model = LSTM().to(device)\n",
    "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "lstm_model.eval()\n",
    "\n",
    "length = 18\n",
    "detect_cls = 1\n",
    "\n",
    "lstm_model.eval()\n",
    "dataset = []\n",
    "status = 'None'\n",
    "prev_action = None\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "poses = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "xyz_list_list = []\n",
    "status_dict = {0: 'normal', 1: 'fighting', 2: 'lying', 3: 'smoking'}\n",
    "rec_start_action = \"REC_ON\"\n",
    "rec_end_action = \"REC_OFF\"\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    RuntimeError(\"카메라 열기 실패\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:  # 프레임 읽기 실패 시 종료\n",
    "        break\n",
    "\n",
    "    # 프레임 크기 조정 (선택 사항, YOLO 모델에 따라 필요)\n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "\n",
    "    # Mediapipe 포즈 추출\n",
    "    results = poses.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    xyz_list = []\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "\n",
    "        # 포즈 랜드마크 추출 및 그리기\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            xyz_list.append(landmark.x)\n",
    "            xyz_list.append(landmark.y)\n",
    "            xyz_list.append(landmark.z)\n",
    "\n",
    "        # YOLO 박스 예측\n",
    "        box_results = yolo_model.predict(frame, conf=0.6, verbose=False, show=False)[0].boxes\n",
    "        boxes = box_results.xyxy.cpu().tolist()\n",
    "        box_class = box_results.cls.cpu().tolist()\n",
    "\n",
    "        p1x1, p1y1, p1x2, p1y2 = 0, 0, 0, 0\n",
    "        p2x1, p2y1, p2x2, p2y2 = 0, 0, 0, 0\n",
    "        for idx, cls in enumerate(box_class):\n",
    "            if int(cls) == detect_cls:\n",
    "                p1x1, p1y1, p1x2, p1y2 = map(int, boxes[0])\n",
    "                if len(boxes) > 1:\n",
    "                    p2x1, p2y1, p2x2, p2y2 = map(int, boxes[1])\n",
    "\n",
    "                break\n",
    "\n",
    "        # YOLO 박스 좌표 정규화 후 추가\n",
    "        xyz_list.extend([abs(p1x1 - p2x1) / 640, abs(p1x2 - p2x2) / 640, abs(p1y1 - p2y1) / 640, abs(p1y2 - p2y2) / 640])\n",
    "        xyz_list_list.append(xyz_list)\n",
    "\n",
    "    # 시퀀스 길이에 도달하면 LSTM 예측 수행\n",
    "    if len(xyz_list_list) == length:\n",
    "        dataset = [{'key': 0, 'value': xyz_list_list}]  # 임시 라벨 0 사용\n",
    "        dataset = MyDataset(dataset)\n",
    "        dataset_loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "        for data, _ in dataset_loader:\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                result = lstm_model(data)\n",
    "                _, out = torch.max(result, 1)\n",
    "                status = status_dict.get(out.item(), 'Unknown')\n",
    "\n",
    "        xyz_list_list = []  # 시퀀스 초기화\n",
    "\n",
    "    if status != \"normal\" and status != \"None\":\n",
    "        if prev_action != rec_start_action:\n",
    "            rec_socket.send(rec_start_action.encode(\"utf-8\"))\n",
    "            prev_action = rec_start_action\n",
    "    else:\n",
    "        if prev_action != rec_end_action:\n",
    "            rec_socket.send(rec_end_action.encode(\"utf-8\"))\n",
    "            prev_action = rec_end_action\n",
    "\n",
    "    # 상태 텍스트 표시\n",
    "    cv2.putText(frame, status, (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (255, 0, 0), 2)\n",
    "\n",
    "    # 프레임 표시\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # 'q' 키로 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 리소스 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hosbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
