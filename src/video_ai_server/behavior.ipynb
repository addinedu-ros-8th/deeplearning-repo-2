{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743647376.039775    4807 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743647376.048070    4807 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743647376.074019    4807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743647376.074081    4807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743647376.074085    4807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743647376.074086    4807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your system is cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import platform\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch.cuda import empty_cache\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    print(\"your system is mac os\")\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "else:\n",
    "    print(\"your system is cuda\")\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/pepsi/dev_ws/mldl/Training/datasets/person'\n",
    "model_name = \"yolov8n.pt\"\n",
    "\n",
    "classes = ['person']\n",
    "nc = len(classes)\n",
    "yaml_file = 'data.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train' : data_dir + '/train/',\n",
    "    'val' : data_dir + '/valid/',\n",
    "    'test' : data_dir + '/test/',\n",
    "    'nc' : nc,\n",
    "    'names' : classes,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "with open(data_dir + '/' + yaml_file, 'wt') as fw:\n",
    "    yaml.dump(data, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'names': ['person'], 'nc': 1, 'test': '/home/pepsi/dev_ws/mldl/Training/datasets/person/test/', 'train': '/home/pepsi/dev_ws/mldl/Training/datasets/person/train/', 'val': '/home/pepsi/dev_ws/mldl/Training/datasets/person/valid/'}\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + '/' + yaml_file, 'rt') as fr:\n",
    "    d = yaml.safe_load(fr)\n",
    "    print(type(d))\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = 100\n",
    "patience = 30\n",
    "batch = 32\n",
    "imgsz = 640\n",
    "LR = 0.001\n",
    "optimizer = 'AdamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO(model_name).to(device)\n",
    "\n",
    "yolo_model.train(data=data_dir + '/' + yaml_file,\n",
    "            epochs = train_epoch,\n",
    "            patience=patience,\n",
    "            batch=batch,\n",
    "            imgsz = imgsz,\n",
    "            optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_best_model_path = '/Users/wjsong/dev_ws/Hosbot/runs/detect/train/weights/best.pt'\n",
    "\n",
    "YOLO_best_model = YOLO(YOLO_best_model_path).to(device)\n",
    "metrics = YOLO_best_model.val()\n",
    "\n",
    "for label, ap in zip(classes, metrics.box.maps):\n",
    "    print(label, ':', ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "yolo_best_model = '/home/pepsi/dev_ws/mldl/Training/runs/detect/train3/weights/best.pt'\n",
    "yolo_model = YOLO(yolo_best_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_YOLO_box(img, yolo_model, detect_cls):\n",
    "    box_results = yolo_model.predict(img, conf=0.6, verbose=False, show=False)\n",
    "    box_results = box_results[0].boxes\n",
    "\n",
    "    boxes = box_results.xyxy.cpu().tolist()\n",
    "    box_class = box_results.cls.cpu().tolist()\n",
    "\n",
    "    p1x1, p1y1, p1x2, p1y2 = 0, 0, 0, 0\n",
    "    p2x1, p2y1, p2x2, p2y2 = 0, 0, 0, 0\n",
    "    for idx, cls in enumerate(box_class):\n",
    "        if int(cls) == detect_cls:\n",
    "            p1x1, p1y1, p1x2, p1y2 = boxes[0]\n",
    "            p1x1, p1y1, p1x2, p1y2 = int(p1x1), int(p1y1), int(p1x2), int(p1y2)\n",
    "\n",
    "            if len(boxes) > 1:\n",
    "                p2x1, p2y1, p2x2, p2y2 = boxes[1]\n",
    "                p2x1, p2y1, p2x2, p2y2 = int(p2x1), int(p2y1), int(p2x2), int(p2y2)\n",
    "\n",
    "    return p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_landmarks(results):\n",
    "    xyz_list = []\n",
    "    for landmark in results.pose_landmarks.landmark:\n",
    "        xyz_list.append(landmark.x)\n",
    "        xyz_list.append(landmark.y)\n",
    "        xyz_list.append(landmark.z)\n",
    "    return xyz_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_xyz_list_list(xyz_list_list, p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2, xyz_list):\n",
    "    xyz_list.append(abs(p1x1 - p2x1) / 640)\n",
    "    xyz_list.append(abs(p1x2 - p2x2) / 640)\n",
    "    xyz_list.append(abs(p1y1 - p2y1) / 640)\n",
    "    xyz_list.append(abs(p1y2 - p2y2) / 640)\n",
    "    xyz_list_list.append(xyz_list)\n",
    "\n",
    "    return xyz_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(mp_pose, video_path, detect_cls):\n",
    "    xyz_list_list = []\n",
    "    poses = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "            if ret == True:\n",
    "                xyz_list = []\n",
    "                img = cv2.resize(img, (640, 640))\n",
    "                results = poses.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                if not results.pose_landmarks: continue\n",
    "\n",
    "                xyz_list = get_pose_landmarks(results)\n",
    "                p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2 = get_YOLO_box(img, yolo_model, detect_cls)\n",
    "\n",
    "                if (p1x1 == 0 and p1y1 == 0 and p1x2 == 0 and p1y2== 0) and (p2x1 == 0 and p2y1 == 0 and p2x2 == 0 and p2y2== 0): continue\n",
    "\n",
    "                xyz_list_list = append_data_xyz_list_list(xyz_list_list, p1x1, p1y1, p1x2, p1y2, p2x1, p2y1, p2x2, p2y2, xyz_list)\n",
    "\n",
    "                cv2.waitKey(1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return xyz_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743645700.343415    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645700.347369    9428 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645700.442730    9416 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645700.478107    9420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "  6%|▋         | 1/16 [00:29<07:23, 29.58s/it]I0000 00:00:1743645729.931986    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645729.933269    9456 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645730.029043    9449 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645730.085862    9451 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 12%|█▎        | 2/16 [00:54<06:14, 26.76s/it]I0000 00:00:1743645754.722597    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645754.728267    9505 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645754.859602    9492 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645754.923762    9501 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 19%|█▉        | 3/16 [01:21<05:47, 26.72s/it]I0000 00:00:1743645781.398222    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645781.399664    9536 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645781.506295    9525 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645781.556173    9523 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 25%|██▌       | 4/16 [01:46<05:14, 26.21s/it]I0000 00:00:1743645806.824390    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645806.825812    9566 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645806.936437    9557 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645806.992166    9558 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 31%|███▏      | 5/16 [02:13<04:49, 26.35s/it]I0000 00:00:1743645833.428164    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645833.432993    9596 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645833.552935    9583 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645833.586461    9582 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 38%|███▊      | 6/16 [02:38<04:20, 26.05s/it]I0000 00:00:1743645858.884783    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645858.886601    9628 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645859.095049    9625 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645859.215954    9618 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 44%|████▍     | 7/16 [02:59<03:38, 24.33s/it]I0000 00:00:1743645879.676640    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645879.678360    9691 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645879.778402    9680 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645879.815782    9684 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 50%|█████     | 8/16 [03:31<03:33, 26.68s/it]I0000 00:00:1743645911.376703    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645911.379580    9731 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645911.492085    9725 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645911.600778    9724 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 56%|█████▋    | 9/16 [03:57<03:05, 26.56s/it]I0000 00:00:1743645937.687522    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645937.689013    9763 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645937.801223    9750 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645937.836917    9754 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 62%|██████▎   | 10/16 [04:18<02:30, 25.04s/it]I0000 00:00:1743645959.311101    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645959.312949    9791 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645959.424953    9783 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645959.479860    9779 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 69%|██████▉   | 11/16 [04:40<01:59, 23.87s/it]I0000 00:00:1743645980.539852    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743645980.541797    9819 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743645980.660911    9807 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743645980.708918    9805 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 75%|███████▌  | 12/16 [05:04<01:36, 24.02s/it]I0000 00:00:1743646004.887315    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743646004.888993    9859 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743646004.977723    9846 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743646005.043851    9852 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 81%|████████▏ | 13/16 [05:32<01:15, 25.11s/it]I0000 00:00:1743646032.527744    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743646032.529176    9893 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743646032.663984    9882 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743646032.821170    9886 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 88%|████████▊ | 14/16 [05:55<00:49, 24.57s/it]I0000 00:00:1743646055.834617    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743646055.835940    9923 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743646055.983323    9912 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743646056.125006    9917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      " 94%|█████████▍| 15/16 [06:30<00:27, 27.76s/it]I0000 00:00:1743646090.982342    5760 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743646090.983952    9953 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1743646091.100997    9943 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743646091.163363    9950 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|██████████| 16/16 [06:59<00:00, 26.22s/it]\n"
     ]
    }
   ],
   "source": [
    "Video_path = './datasets/pose/train2'\n",
    "video_name_list = os.listdir(Video_path)\n",
    "dataset = []\n",
    "length = 20\n",
    "detect_cls = 0\n",
    "\n",
    "for video_name in tqdm(video_name_list):\n",
    "    if 'normal' in video_name: label = 0\n",
    "    elif 'fighting' in video_name: label = 1\n",
    "    elif 'lying' in video_name: label = 2\n",
    "    elif 'smoking' in video_name: label = 3\n",
    "\n",
    "    pose_data = generate_dataset(mp_pose, '{}/{}'.format(Video_path, video_name), detect_cls)\n",
    "\n",
    "    for idx in range(0, len(pose_data), int(length)):\n",
    "        seq_list = pose_data[idx : idx + length]\n",
    "        if len(seq_list) == length:\n",
    "            dataset.append({'key' : label, 'value': seq_list})\n",
    "\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data [0.4049217700958252, 0.3418542146682739, -0.09563499689102173, 0.40718451142311096, 0.3306730389595032, -0.08019842952489853, 0.4082857668399811, 0.3302232027053833, -0.08024564385414124, 0.4095185399055481, 0.3297785520553589, -0.08028195798397064, 0.4030199944972992, 0.3291739821434021, -0.10445760190486908, 0.4002353549003601, 0.32796043157577515, -0.10442928969860077, 0.39815306663513184, 0.32661283016204834, -0.10451358556747437, 0.4051211476325989, 0.32861778140068054, 0.017215870320796967, 0.3886374235153198, 0.32688695192337036, -0.095341756939888, 0.4048277735710144, 0.3509773015975952, -0.05451038479804993, 0.39755570888519287, 0.3493660092353821, -0.08712806552648544, 0.3967532217502594, 0.37723299860954285, 0.11964359134435654, 0.36059829592704773, 0.37002426385879517, -0.11708519607782364, 0.4187869429588318, 0.4673067629337311, 0.13907867670059204, 0.38053032755851746, 0.4461829662322998, -0.20440462231636047, 0.4292142987251282, 0.4716995358467102, 0.06025876849889755, 0.41386786103248596, 0.45097899436950684, -0.28963619470596313, 0.43063464760780334, 0.4771321415901184, 0.050951749086380005, 0.4222322106361389, 0.4568910002708435, -0.31880053877830505, 0.435577929019928, 0.46518611907958984, 0.03639668598771095, 0.42372846603393555, 0.44744986295700073, -0.3159940838813782, 0.4343230128288269, 0.46361979842185974, 0.05125141143798828, 0.4186653196811676, 0.44653281569480896, -0.28934839367866516, 0.41189470887184143, 0.542675793170929, 0.056231699883937836, 0.39880573749542236, 0.5456168055534363, -0.05619538575410843, 0.4112047553062439, 0.6472995281219482, 0.0454898476600647, 0.4181601405143738, 0.6476160883903503, -0.04437386244535446, 0.4023488461971283, 0.7563448548316956, 0.09616885334253311, 0.4060000479221344, 0.7586154341697693, 0.021470941603183746, 0.3944236636161804, 0.7730159759521484, 0.09503337740898132, 0.39919784665107727, 0.7764110565185547, 0.02444138377904892, 0.42489784955978394, 0.7889641523361206, 0.01626698672771454, 0.429526150226593, 0.7874420881271362, -0.05678265169262886, 0.0890625, 0.040625, 0.0140625, 0.0375]\n",
      "input data length 103\n"
     ]
    }
   ],
   "source": [
    "print('input data', dataset[0]['value'][0])\n",
    "print('input data length', len(dataset[0]['value'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq_list):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for dic in seq_list:\n",
    "            self.y.append(dic['key'])\n",
    "            self.X.append(dic['value'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m split_ratio = [\u001b[32m0.7\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_len = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m) * split_ratio[\u001b[32m0\u001b[39m])\n\u001b[32m      3\u001b[39m val_len = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset) * split_ratio[\u001b[32m1\u001b[39m])\n\u001b[32m      4\u001b[39m test_len = \u001b[38;5;28mlen\u001b[39m(dataset) - train_len - val_len\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "split_ratio = [0.7, 0.2, 0.1]\n",
    "train_len = int(len(dataset) * split_ratio[0])\n",
    "val_len = int(len(dataset) * split_ratio[1])\n",
    "test_len = len(dataset) - train_len - val_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dataset = MyDataset(\u001b[43mdataset\u001b[49m)\n\u001b[32m      2\u001b[39m train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n\u001b[32m      4\u001b[39m train_loader = DataLoader(train_data, batch_size=\u001b[32m8\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(dataset)\n",
    "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8)\n",
    "val_loader = DataLoader(valid_data, batch_size=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(103, 128, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(256)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(256, 64, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm2 = nn.LayerNorm(128)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm3 = nn.LSTM(128, 32, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm3 = nn.LayerNorm(64)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "        self.attention = nn.Linear(64, 1)\n",
    "        self.fc = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.layer_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
    "        x = torch.sum(attention_weights * x, dim=1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    global net, loss_fn, optim\n",
    "    net = LSTM().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optim = AdamW(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_epoch():\n",
    "    global epoch_cnt\n",
    "    epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_log():\n",
    "    global iter_log, tloss_log, tacc_log, vloss_log, vacc_log, log_stack, time_log\n",
    "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
    "    log_stack, time_log = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_train_log(_tloss, _tacc, _time):\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    tacc_log.append(_tacc)\n",
    "    iter_log.append(epoch_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_valid_log(_vloss, _vacc):\n",
    "    vloss_log.append(_vloss)\n",
    "    vacc_log.append(_vacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last(log_list):\n",
    "    if len(log_list) > 0:\n",
    "        return log_list[len(log_list) - 1]\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log():\n",
    "    train_loss = round(float(last(tloss_log)), 3)\n",
    "    train_acc = round(float(last(tacc_log)), 3)\n",
    "    val_loss = round(float(last(vloss_log)), 3)\n",
    "    val_acc = round(float(last(vacc_log)), 3)\n",
    "    time_spent = round(float(last(time_log)), 3)\n",
    "\n",
    "    log_str = 'Epoch: {:3}| T_Loss {:5} | T_acc {:5}| V_Loss {:5}| V_acc {:5} | {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)\n",
    "\n",
    "    log_stack.append(log_str) #프린트 준비\n",
    "\n",
    "    for idx in reversed(range(len(log_stack))):\n",
    "        print(log_stack[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "def epoch_not_finished():\n",
    "    return epoch_cnt < maximum_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(data_loader, mode='train'):\n",
    "    global epoch_cnt\n",
    "\n",
    "    iter_loss, iter_acc, last_grad_performed = [], [], False\n",
    "\n",
    "    for _data, _label in data_loader:\n",
    "        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)\n",
    "\n",
    "        if mode == 'train' : net.train()\n",
    "        else: net.eval()\n",
    "\n",
    "        result = net(data)\n",
    "        _, out = torch.max(result, 1)\n",
    "\n",
    "        loss = loss_fn(result, label)\n",
    "        iter_loss.append(loss.item())\n",
    "\n",
    "        if mode == 'train':\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            last_grad_performed = True\n",
    "\n",
    "        acc_partial = (out == label).float().sum()\n",
    "        acc_partial = acc_partial / len(label)\n",
    "        iter_acc.append(acc_partial.item())\n",
    "\n",
    "\n",
    "    if last_grad_performed:\n",
    "        epoch_cnt += 1\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "    return np.average(iter_loss), np.average(iter_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "maximum_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  44| T_Loss 0.323 | T_acc 0.842| V_Loss 0.343| V_acc 0.747 | 0.273\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  45| T_Loss 0.418 | T_acc 0.707| V_Loss 0.307| V_acc 0.726 | 0.269\n",
      "Epoch:  44| T_Loss 0.323 | T_acc 0.842| V_Loss 0.343| V_acc 0.747 | 0.273\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  46| T_Loss 0.365 | T_acc 0.812| V_Loss  0.33| V_acc 0.768 | 0.276\n",
      "Epoch:  45| T_Loss 0.418 | T_acc 0.707| V_Loss 0.307| V_acc 0.726 | 0.269\n",
      "Epoch:  44| T_Loss 0.323 | T_acc 0.842| V_Loss 0.343| V_acc 0.747 | 0.273\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  47| T_Loss 0.321 | T_acc 0.824| V_Loss 0.338| V_acc 0.768 | 0.275\n",
      "Epoch:  46| T_Loss 0.365 | T_acc 0.812| V_Loss  0.33| V_acc 0.768 | 0.276\n",
      "Epoch:  45| T_Loss 0.418 | T_acc 0.707| V_Loss 0.307| V_acc 0.726 | 0.269\n",
      "Epoch:  44| T_Loss 0.323 | T_acc 0.842| V_Loss 0.343| V_acc 0.747 | 0.273\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  48| T_Loss  0.39 | T_acc  0.77| V_Loss 0.317| V_acc 0.747 | 0.303\n",
      "Epoch:  47| T_Loss 0.321 | T_acc 0.824| V_Loss 0.338| V_acc 0.768 | 0.275\n",
      "Epoch:  46| T_Loss 0.365 | T_acc 0.812| V_Loss  0.33| V_acc 0.768 | 0.276\n",
      "Epoch:  45| T_Loss 0.418 | T_acc 0.707| V_Loss 0.307| V_acc 0.726 | 0.269\n",
      "Epoch:  44| T_Loss 0.323 | T_acc 0.842| V_Loss 0.343| V_acc 0.747 | 0.273\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  49| T_Loss 0.389 | T_acc 0.752| V_Loss 0.443| V_acc 0.747 | 0.291\n",
      "Epoch:  48| T_Loss  0.39 | T_acc  0.77| V_Loss 0.317| V_acc 0.747 | 0.303\n",
      "Epoch:  47| T_Loss 0.321 | T_acc 0.824| V_Loss 0.338| V_acc 0.768 | 0.275\n",
      "Epoch:  46| T_Loss 0.365 | T_acc 0.812| V_Loss  0.33| V_acc 0.768 | 0.276\n",
      "Epoch:  45| T_Loss 0.418 | T_acc 0.707| V_Loss 0.307| V_acc 0.726 | 0.269\n",
      "Epoch:  44| T_Loss 0.323 | T_acc 0.842| V_Loss 0.343| V_acc 0.747 | 0.273\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "Epoch:  50| T_Loss 0.446 | T_acc 0.743| V_Loss 0.308| V_acc 0.705 | 0.279\n",
      "Epoch:  49| T_Loss 0.389 | T_acc 0.752| V_Loss 0.443| V_acc 0.747 | 0.291\n",
      "Epoch:  48| T_Loss  0.39 | T_acc  0.77| V_Loss 0.317| V_acc 0.747 | 0.303\n",
      "Epoch:  47| T_Loss 0.321 | T_acc 0.824| V_Loss 0.338| V_acc 0.768 | 0.275\n",
      "Epoch:  46| T_Loss 0.365 | T_acc 0.812| V_Loss  0.33| V_acc 0.768 | 0.276\n",
      "Epoch:  45| T_Loss 0.418 | T_acc 0.707| V_Loss 0.307| V_acc 0.726 | 0.269\n",
      "Epoch:  44| T_Loss 0.323 | T_acc 0.842| V_Loss 0.343| V_acc 0.747 | 0.273\n",
      "Epoch:  43| T_Loss 0.305 | T_acc 0.865| V_Loss 0.415| V_acc 0.747 | 0.275\n",
      "Epoch:  42| T_Loss 0.341 | T_acc 0.818| V_Loss 0.319| V_acc 0.768 | 0.296\n",
      "Epoch:  41| T_Loss 0.316 | T_acc 0.824| V_Loss 0.334| V_acc 0.747 | 0.361\n",
      "Epoch:  40| T_Loss 0.326 | T_acc  0.83| V_Loss 0.346| V_acc 0.726 | 0.624\n",
      "Epoch:  39| T_Loss 0.339 | T_acc 0.824| V_Loss 0.327| V_acc 0.768 | 0.447\n",
      "Epoch:  38| T_Loss 0.361 | T_acc 0.796| V_Loss 0.327| V_acc 0.789 | 0.429\n",
      "Epoch:  37| T_Loss 0.371 | T_acc 0.773| V_Loss 0.324| V_acc 0.768 | 0.351\n",
      "Epoch:  36| T_Loss  0.38 | T_acc 0.806| V_Loss 0.306| V_acc  0.81 | 0.272\n",
      "Epoch:  35| T_Loss 0.387 | T_acc 0.788| V_Loss  0.32| V_acc 0.747 | 0.273\n",
      "Epoch:  34| T_Loss 0.379 | T_acc  0.77| V_Loss 0.327| V_acc 0.726 | 0.273\n",
      "Epoch:  33| T_Loss 0.401 | T_acc 0.764| V_Loss 0.339| V_acc 0.768 | 0.276\n",
      "Epoch:  32| T_Loss 0.413 | T_acc 0.731| V_Loss 0.324| V_acc 0.812 | 0.293\n",
      "Epoch:  31| T_Loss 0.467 | T_acc 0.737| V_Loss 0.315| V_acc  0.83 |  0.28\n",
      "Epoch:  30| T_Loss 0.454 | T_acc 0.701| V_Loss 0.527| V_acc 0.702 | 0.281\n",
      "Epoch:  29| T_Loss 0.425 | T_acc 0.746| V_Loss 0.349| V_acc 0.747 | 0.285\n",
      "Epoch:  28| T_Loss 0.503 | T_acc 0.689| V_Loss 0.312| V_acc 0.875 |  1.08\n",
      "Epoch:  27| T_Loss 0.377 | T_acc 0.788| V_Loss 0.312| V_acc 0.768 |   0.9\n",
      "Epoch:  26| T_Loss 0.408 | T_acc 0.695| V_Loss  0.29| V_acc 0.833 | 0.938\n",
      "Epoch:  25| T_Loss 0.386 | T_acc 0.746| V_Loss 0.309| V_acc 0.726 | 0.932\n",
      "Epoch:  24| T_Loss 0.397 | T_acc 0.735| V_Loss 0.304| V_acc 0.768 | 0.845\n",
      "Epoch:  23| T_Loss 0.396 | T_acc 0.752| V_Loss 0.303| V_acc 0.768 | 0.892\n",
      "Epoch:  22| T_Loss   0.4 | T_acc  0.74| V_Loss 0.304| V_acc 0.747 | 0.879\n",
      "Epoch:  21| T_Loss 0.403 | T_acc 0.761| V_Loss 0.302| V_acc  0.75 | 0.836\n",
      "Epoch:  20| T_Loss 0.407 | T_acc 0.761| V_Loss 0.299| V_acc 0.833 | 0.812\n",
      "Epoch:  19| T_Loss 0.444 | T_acc 0.707| V_Loss 0.299| V_acc 0.833 | 1.297\n",
      "Epoch:  18| T_Loss 0.514 | T_acc  0.72| V_Loss 0.335| V_acc 0.747 | 0.485\n",
      "Epoch:  17| T_Loss 0.398 | T_acc 0.751| V_Loss 0.325| V_acc 0.664 | 0.501\n",
      "Epoch:  16| T_Loss  0.42 | T_acc 0.743| V_Loss 0.296| V_acc 0.833 | 0.495\n",
      "Epoch:  15| T_Loss 0.507 | T_acc 0.713| V_Loss 0.329| V_acc 0.726 | 0.504\n",
      "Epoch:  14| T_Loss 0.485 | T_acc 0.719| V_Loss  0.99| V_acc 0.598 | 0.567\n",
      "Epoch:  13| T_Loss 0.403 | T_acc   0.8| V_Loss  0.35| V_acc 0.705 | 0.486\n",
      "Epoch:  12| T_Loss 0.415 | T_acc 0.695| V_Loss 0.301| V_acc 0.812 |  0.49\n",
      "Epoch:  11| T_Loss 0.427 | T_acc 0.758| V_Loss 0.328| V_acc 0.747 | 0.473\n",
      "Epoch:  10| T_Loss 0.402 | T_acc 0.746| V_Loss 0.329| V_acc 0.685 | 0.489\n",
      "Epoch:   9| T_Loss  0.41 | T_acc 0.755| V_Loss 0.304| V_acc 0.726 | 0.498\n",
      "Epoch:   8| T_Loss 0.407 | T_acc 0.761| V_Loss 0.301| V_acc 0.833 | 0.743\n",
      "Epoch:   7| T_Loss 0.425 | T_acc 0.695| V_Loss   0.3| V_acc 0.833 | 0.958\n",
      "Epoch:   6| T_Loss 0.442 | T_acc 0.761| V_Loss 0.311| V_acc 0.747 | 1.111\n",
      "Epoch:   5| T_Loss 0.488 | T_acc 0.683| V_Loss 0.307| V_acc 0.833 | 0.888\n",
      "Epoch:   4| T_Loss 0.446 | T_acc 0.758| V_Loss  0.34| V_acc 0.747 | 0.888\n",
      "Epoch:   3| T_Loss  0.49 | T_acc 0.719| V_Loss 0.349| V_acc  0.81 | 0.929\n",
      "Epoch:   2| T_Loss 0.518 | T_acc 0.758| V_Loss 0.364| V_acc 0.747 | 0.967\n",
      "Epoch:   1| T_Loss 1.021 | T_acc 0.606| V_Loss 0.508| V_acc 0.792 | 0.473\n",
      "\n",
      " Training completed!\n"
     ]
    }
   ],
   "source": [
    "while epoch_not_finished():\n",
    "    start_time = time.time()\n",
    "    tloss, tacc = epoch(train_loader, mode='train')\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, tacc, time_taken)\n",
    "    with torch.no_grad():\n",
    "        vloss, vacc = epoch(val_loader, mode= 'val')\n",
    "        record_valid_log(vloss, vacc)\n",
    "    print_log()\n",
    "\n",
    "print('\\n Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.875\n",
      "Test Loss: 0.4573\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_loss, test_acc = epoch(test_loader, mode='test')\n",
    "    test_acc = round(test_acc, 4)\n",
    "    test_loss = round(test_loss, 4)\n",
    "    print('Test Acc: {}'.format(test_acc))\n",
    "    print('Test Loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/lstm_model.pth'\n",
    "\n",
    "torch.save(net.state_dict(), model_path)\n",
    "print('모델이 저장되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 성공적으로 불러와졌습니다.\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/lstm_model.pth'\n",
    "\n",
    "lstm_model = LSTM().to(device)\n",
    "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "lstm_model.eval()\n",
    "print(\"모델이 성공적으로 불러와졌습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743646739.685445    4471 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743646739.690159    4733 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743646739.778149    4721 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743646739.820520    4723 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "length = 40\n",
    "detect_cls = 0\n",
    "\n",
    "lstm_model.eval()\n",
    "dataset = []\n",
    "status = 'None'\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "poses = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "xyz_list_list = []\n",
    "status_dict = {0: 'normal', 1: 'fighting', 2: 'lying', 3: 'smoking'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    RuntimeError(\"카메라 열기 실패\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:  # 프레임 읽기 실패 시 종료\n",
    "        break\n",
    "\n",
    "    # 프레임 크기 조정 (선택 사항, YOLO 모델에 따라 필요)\n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "\n",
    "    # Mediapipe 포즈 추출\n",
    "    results = poses.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    xyz_list = []\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        continue\n",
    "\n",
    "    # 포즈 랜드마크 추출 및 그리기\n",
    "    for landmark in results.pose_landmarks.landmark:\n",
    "        xyz_list.append(landmark.x)\n",
    "        xyz_list.append(landmark.y)\n",
    "        xyz_list.append(landmark.z)\n",
    "\n",
    "    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # YOLO 박스 예측\n",
    "    box_results = yolo_model.predict(frame, conf=0.6, verbose=False, show=False)[0].boxes\n",
    "    boxes = box_results.xyxy.cpu().tolist()\n",
    "    box_class = box_results.cls.cpu().tolist()\n",
    "\n",
    "    p1x1, p1y1, p1x2, p1y2 = 0, 0, 0, 0\n",
    "    p2x1, p2y1, p2x2, p2y2 = 0, 0, 0, 0\n",
    "    for idx, cls in enumerate(box_class):\n",
    "        if int(cls) == detect_cls:\n",
    "            p1x1, p1y1, p1x2, p1y2 = map(int, boxes[0])\n",
    "            if len(boxes) > 1:\n",
    "                p2x1, p2y1, p2x2, p2y2 = map(int, boxes[1])\n",
    "            cv2.rectangle(frame, (p1x1, p1y1), (p1x2, p1y2), (0, 255, 0), 2)\n",
    "            cv2.rectangle(frame, (p2x1, p2y1), (p2x2, p2y2), (0, 255, 0), 2)\n",
    "            break\n",
    "\n",
    "    if (p1x1 == 0 and p1y1 == 0 and p1x2 == 0 and p1y2== 0) and (p2x1 == 0 and p2y1 == 0 and p2x2 == 0 and p2y2== 0):\n",
    "        continue\n",
    "\n",
    "    # YOLO 박스 좌표 정규화 후 추가\n",
    "    xyz_list.extend([abs(p1x1 - p2x1) / 640, abs(p1x2 - p2x2) / 640, abs(p1y1 - p2y1) / 640, abs(p1y2 - p2y2) / 640])\n",
    "    xyz_list_list.append(xyz_list)\n",
    "\n",
    "    # 시퀀스 길이에 도달하면 LSTM 예측 수행\n",
    "    if len(xyz_list_list) == length:\n",
    "        dataset = [{'key': 0, 'value': xyz_list_list}]  # 임시 라벨 0 사용\n",
    "        dataset = MyDataset(dataset)\n",
    "        dataset_loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "        for data, _ in dataset_loader:\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                result = lstm_model(data)\n",
    "                _, out = torch.max(result, 1)\n",
    "                print(out)\n",
    "                status = status_dict.get(out.item(), 'Unknown')\n",
    "\n",
    "        xyz_list_list = []  # 시퀀스 초기화\n",
    "\n",
    "    # 상태 텍스트 표시\n",
    "    cv2.putText(frame, status, (10, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (255, 0, 0), 2)\n",
    "\n",
    "    # 프레임 표시\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # 'q' 키로 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 리소스 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hosbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
